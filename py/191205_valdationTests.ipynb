{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from astropy.constants import c\n",
    "from scipy import interpolate\n",
    "import pickle\n",
    "from mpfit import mpfit\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from waveCal import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LFC\n",
    "lfc_files = glob('/mnt/home/lzhao/ceph/lfc5*/LFC_*.fits')\n",
    "ckpt_files = glob('/mnt/home/lzhao/ceph/ckpt5*/LFC_19*.npy')\n",
    "print(len(lfc_files))\n",
    "lfc_files, lfc_times = sortFiles(lfc_files, get_mjd=True)\n",
    "ckpt_files = sortFiles(ckpt_files)\n",
    "num_lfc_files = len(lfc_files)\n",
    "\n",
    "hdus = fits.open(lfc_files[0])\n",
    "t_spec = hdus[1].data['spectrum'].copy()\n",
    "t_errs = hdus[1].data['uncertainty'].copy()\n",
    "t_mask = hdus[1].data['pixel_mask'].copy()\n",
    "hdus.close()\n",
    "nord, npix = t_spec.shape\n",
    "\n",
    "lfc_orders = range(45,76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ThAr\n",
    "thar_files = glob('/mnt/home/lzhao/ceph/thar5*/ThAr_*.fits')\n",
    "thid_files  = glob('/mnt/home/lzhao/ceph/thid5*/ThAr_*.thid')\n",
    "print(len(thar_files))\n",
    "thar_files, thar_times = sortFiles(thar_files, get_mjd=True)\n",
    "thar_files = thar_files[1:] # First file is from before LFCs\n",
    "thar_times = thar_times[1:]\n",
    "thid_files = sortFiles(thid_files) [1:]\n",
    "num_thar_files = len(thar_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Make sure validation exposures are not first or last exposure\n",
    "valid_idx = np.random.choice(num_lfc_files-2, num_lfc_files//10, replace=False)+1\n",
    "\n",
    "lfc_train = np.delete(ckpt_files,valid_idx)\n",
    "lfc_times_train = np.delete(lfc_times,valid_idx)\n",
    "time_sort = np.argsort(lfc_times_train)\n",
    "lfc_train = lfc_train[time_sort]\n",
    "lfc_times_train = lfc_times_train[time_sort]\n",
    "\n",
    "lfc_valid = ckpt_files[valid_idx]\n",
    "lfc_times_valid = lfc_times[valid_idx]\n",
    "time_sort = np.argsort(lfc_times_valid)\n",
    "lfc_valid = lfc_valid[time_sort]\n",
    "lfc_times_valid = lfc_times_valid[time_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "valid_idx = np.random.choice(num_thar_files-2, num_thar_files//10, replace=False)+1\n",
    "\n",
    "thar_train = np.delete(thid_files,valid_idx)\n",
    "thar_times_train = np.delete(thar_times,valid_idx)\n",
    "time_sort = np.argsort(thar_times_train)\n",
    "thar_train = thar_train[time_sort]\n",
    "thar_times_train = thar_times_train[time_sort]\n",
    "\n",
    "thar_valid = thid_files[valid_idx]\n",
    "thar_times_valid = thar_times[valid_idx]\n",
    "time_sort = np.argsort(thar_times_valid)\n",
    "thar_valid = thar_train[time_sort]\n",
    "thar_times_valid = thar_times_valid[time_sort]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Save Patch Dictionaries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ckpt_patch_train = patchAndDenoise(lfc_train, file_times=lfc_times_train,\n",
    "    K=2, num_iters=50, return_iters=False, running_window=9,\n",
    "    line_cutoff=0.5, file_cutoff=0.5, fast_pca=False, verbose=True)\n",
    "#pickle.dump(ckpt_patch_train, open( \"./191205_ckptPatch9_train.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "thid_patch_train = patchAndDenoise(thar_train, file_times=thar_times_train,\n",
    "    K=2, num_iters=50, return_iters=False, running_window=15,\n",
    "    line_cutoff=0.7, file_cutoff=0.7, fast_pca=False, verbose=True)\n",
    "#pickle.dump(thid_patch_train, open( \"./191205_thidPatch15_train.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_patch_train = pickle.load(open('./191205_ckptPatch9_train.pkl','rb'))\n",
    "thid_patch_train = pickle.load(open('./191205_thidPatch15_train.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFC Validation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_xs = evalWaveSol(lfc_times_valid, ckpt_patch_train, t_intp_deg=3)\n",
    "m = ckpt_patch_train['orders'].copy()\n",
    "w = ckpt_patch_train['waves'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc_fits = []\n",
    "lfc_shifts = np.array([],dtype=float)\n",
    "ckpt_x = []\n",
    "ckpt_m = []\n",
    "ckpt_w = []\n",
    "for file_num in tqdm(range(len(lfc_valid))):\n",
    "    file_name = lfc_valid[file_num]\n",
    "    try:\n",
    "        newx,newm,neww,newe = readParams(file_name)\n",
    "    except ValueError as err:\n",
    "        continue\n",
    "    \n",
    "    w_fit = interp_train_and_predict(newx, newm,\n",
    "                                     denoised_xs[file_num], m, w,\n",
    "                                     e=newe, interp_deg=3)\n",
    "    \n",
    "    ckpt_x.append(newx)\n",
    "    ckpt_m.append(newm)\n",
    "    ckpt_w.append(neww)\n",
    "    lfc_fits.append(w_fit)\n",
    "    good_mask = np.isfinite(w_fit)\n",
    "    lfc_shifts = np.concatenate([lfc_shifts,\n",
    "                                 (w_fit[good_mask]-neww[good_mask])/neww[good_mask]*c.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_shift = lfc_shifts.flatten()\n",
    "rv_shift = rv_shift[abs(rv_shift)<25]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'LFC Training and Validation: All {len(lfc_times_valid)} Validation Exposures')\n",
    "plt.xlabel('Predicted - Fit [m/s]')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(rv_shift,50);\n",
    "plt.axvline(np.mean(rv_shift),color='r',label='Mean: {:.3} m/s'.format(np.mean(rv_shift)))\n",
    "plt.axvline(np.median(rv_shift),color='g',label='Median: {:.3} m/s'.format(np.median(rv_shift)))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Figures/191205_lfcTnV.png')\n",
    "print(np.std(rv_shift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.4*2,4.8))\n",
    "plt.xlabel('Time [mjd]')\n",
    "plt.ylabel('PCA Coefficient')\n",
    "for i in lfc_times_valid:\n",
    "    plt.axvline(i,color='.75')\n",
    "plt.axvline(i,color='.75',label='Validation Times')\n",
    "plt.axvline(lfc_times_valid[38],color=sns.color_palette()[4],label='Problem Child')\n",
    "\n",
    "plt.plot(ckpt_patch_train['times'],ckpt_patch_train['ec'][:,0],'.-',color=sns.color_palette()[0],label='EC 0')\n",
    "f = interpolate.interp1d(ckpt_patch_train['times'],ckpt_patch_train['ec'][:,0],kind='cubic',\n",
    "                     bounds_error=False,fill_value=np.nan)\n",
    "x = np.linspace(lfc_times_valid[0],lfc_times_valid[-1],1000)\n",
    "plt.plot(x,f(x),color=sns.color_palette()[2],label='EC 0 Interp')\n",
    "plt.plot(ckpt_patch_train['times'],ckpt_patch_train['ec'][:,1],'.-',color=sns.color_palette()[1],label='EC 1')\n",
    "f = interpolate.interp1d(ckpt_patch_train['times'],ckpt_patch_train['ec'][:,1],kind='cubic',\n",
    "                     bounds_error=False,fill_value=np.nan)\n",
    "plt.plot(x,f(x),color=sns.color_palette()[3],label='EC 1 Interp')\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Figures/191205_intpBad.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Predicted - Fit [m/s]')\n",
    "plt.ylabel('Frequency')\n",
    "colors = sns.color_palette('plasma',len(lfc_times_valid))\n",
    "for i, t in enumerate(lfc_times_valid):\n",
    "    resid = lfc_fits[i] - ckpt_w[i]\n",
    "    rv_shift = resid/ckpt_w[i]*c.value\n",
    "    plt.hist(rv_shift,np.arange(-25,26,2.5),histtype='step',color=colors[i])\n",
    "    \n",
    "resid = lfc_fits[38] - ckpt_w[38]\n",
    "rv_shift = resid/ckpt_w[38]*c.value\n",
    "plt.hist(rv_shift,np.arange(-25,26,2.5),histtype='step',color='r')\n",
    "plt.xlim(-25,25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('LFC Training and Validation: Exp {}'.format(Time(t,format='mjd').isot))\n",
    "plt.xlabel('Predicted - Fit [m/s]')\n",
    "plt.ylabel('Frequency')\n",
    "resid = lfc_fits[1] - ckpt_w[1]\n",
    "rv_shift = resid/ckpt_w[1]*c.value\n",
    "plt.hist(rv_shift,50);\n",
    "plt.axvline(np.nanmean(rv_shift),color='r',label='Mean: {:.3} m/s'.format(np.nanmean(rv_shift)))\n",
    "plt.axvline(np.nanmedian(rv_shift),color='g',label='Median: {:.3} m/s'.format(np.nanmedian(rv_shift)))\n",
    "plt.legend()\n",
    "plt.xlim(-25,25)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./Figures/191205_lfcTnV6.png')\n",
    "print(np.nanstd(rv_shift))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThAr Validation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_xs = evalWaveSol(thar_times_valid, thid_patch_train, t_intp_deg=3)\n",
    "m = thid_patch_train['orders'].copy()\n",
    "w = thid_patch_train['waves'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thar_fits = []\n",
    "thar_shifts = np.array([],dtype=float)\n",
    "thid_x = []\n",
    "thid_m = []\n",
    "thid_w = []\n",
    "for file_num in tqdm(range(len(thar_valid))):\n",
    "    file_name = thar_valid[file_num]\n",
    "    try:\n",
    "        newx,newm,neww = readThid(file_name)\n",
    "    except ValueError as err:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        w_fit = interp_train_and_predict(newx, newm,\n",
    "                                         denoised_xs[file_num], m, w,\n",
    "                                         e=None, interp_deg=3)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    thid_x.append(newx)\n",
    "    thid_m.append(newm)\n",
    "    thid_w.append(neww)\n",
    "    thar_fits.append(w_fit)\n",
    "    good_mask = np.isfinite(w_fit)\n",
    "    thar_shifts = np.concatenate([thar_shifts,\n",
    "                                 (w_fit[good_mask]-neww[good_mask])/neww[good_mask]*c.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_shift = thar_shifts.flatten()\n",
    "innie_mask = abs(rv_shift) < 2000\n",
    "\n",
    "plt.figure()\n",
    "plt.title('ThAr Training and Validation')\n",
    "plt.xlabel('Predicted - Fit [m/s]')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(rv_shift[innie_mask],50);\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Figures/191205_tharTnV.png')\n",
    "print(np.std(rv_shift[innie_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('ThAr Training and Validation: Exp {}'.format(Time(t,format='mjd').isot))\n",
    "plt.xlabel('Predicted - Fit [m/s]')\n",
    "plt.ylabel('Frequency')\n",
    "resid = thar_fits[2] - thid_w[2]\n",
    "rv_shift = resid/thid_w[2]*c.value\n",
    "plt.hist(rv_shift,50);\n",
    "plt.axvline(np.nanmean(rv_shift),color='r',label='Mean: {:.3} m/s'.format(np.nanmean(rv_shift)))\n",
    "plt.axvline(np.nanmedian(rv_shift),color='g',label='Median: {:.3} m/s'.format(np.nanmedian(rv_shift)))\n",
    "plt.legend()\n",
    "plt.xlim(-25,25)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./Figures/191205_tharTnV11.png')\n",
    "print(np.nanstd(rv_shift))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
