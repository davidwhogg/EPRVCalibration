% This document is part of the EPRVCalibration project
% Copyright 2019 the authors. All rights reserved.

% style notes
% -----------
% - use \acronym and use \eprv, \lfc, and so on.

\documentclass[twocolumn]{aastex63}
\usepackage{xcolor}
\newcommand{\lz}[1]{\textcolor{orange}{#1}}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}

% typesetting words
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\name}{\project{Excalibur}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\expres}{\project{\acronym{EXPRES}}}
\newcommand{\eprv}{\acronym{EPRV}}
\newcommand{\lfc}{\acronym{LFC}}

% math shih
\newcommand{\mps}{\mathrm{m\,s^{-1}}}

% margins and page setup, etc
\addtolength{\textheight}{1.00in}
\addtolength{\topmargin}{-0.50in}
\sloppy\sloppypar\raggedbottom\frenchspacing

\begin{document}
\title{\name:
A non-parametric, hierarchical model for a precision spectrograph}

\correspondingauthor{Lily Zhao}
\email{lily.zhao@yale.edu}

\author[0000-0002-3852-3590]{Lily Zhao}
\affil{Yale University, 52 Hillhouse, New Haven, CT 06511, USA}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726 Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Avenue, New York, NY 10011, USA}
\affil{Max-Planck-Institut für Astronomie, Königstuhl 17, D-69117 Heidelberg, Germany}
\affil{Flatiron Institute, Simons Foundation, 162 Fifth Avenue, New York, NY 10010, USA}

\author[0000-0001-9907-7742]{Megan Bedell}
\affil{Flatiron Institute, Simons Foundation, 162 Fifth Avenue, New York, NY 10010, USA}

\author[0000-0003-2221-0861]{Debra A. Fischer}
\affil{Yale University, 52 Hillhouse, New Haven, CT 06511, USA}

\begin{abstract}
\name\ is a hierarchical, non-parametric framework for wavelength calibrating precision spectrographs.  This method makes use of the many, recent hardware improvements in highly-stabilized, extreme-precision radial-velocity spectrographs.  With stabilized spectrographs, the full optical system and detectors can only vary within a tiny range of configurations.  This allows us to use all calibration data to determine the entire space of possible calibrations for a stabilized spectrograph.  A wavelength solution then need only pinpoint where in this low-dimensional calibration space the spectrograph exists.  \name\ also takes advantage of the development of laser-frequency combs or etalons, which generate a dense set of stable calibration points.  This density of points permits the freedom of a non-parametric wavelength solution that can match or adapt to any instrument or detector oddities.  We demonstrate the success of this method with data from a Menlo Systems laser frequency comb taken by \expres, the Extreme Precision Spectrograph.  We find that \name\ returns more accurate wavelength predictions by a factor of 10 than classic polynomial models.  Employing \name\ wavelengths reduced the RMS of RV measurements to a planet fit by \lz{some percent}.
\end{abstract}


% Add official keywords
\keywords{instrumentation: spectrographs -- techniques: spectroscopic -- techniques: radial velocities}

\section{Introduction} 
Extreme precision radial-velocity (\eprv) programs have been incredibly fruitful in finding and characterizing extra-solar planets (CITE THINGS).  These programs typically make use of spectrographs with resolutions on the order of $10^5$, which correspond to line widths on the order of $3000\,\mps$.  Exoplanet science happens at the level of $3\,\mps$ precision, with current instruments hoping to reach $0.1\,\mps$ precision (CITE THINGS).  This requires new spectrographs to be calibrated or stabilized to better than $10^{-4}$ of a pixel (assuming that the spectrograph is well sampled).    Some hardware--software systems, such as ESPRESSO and \expres\ are now reaching these levels (CITE THINGS).

Here we propose to simplify and improve calibration programs for \eprv\ hardware systems with two very simple but novel ideas.  The first flows from the observation that calibration sources---which include lamps \lz{(looks like the ThAr isn't quite good enough actually)}, etalons, and laser-frequency combs (\lfc s)---illuminate the spectrograph with very stable, very dense sets of lines; almost every location in the spectrograph image plane is surrounded by nearby, useful calibration lines.  This recommends a calibration methodology that is \emph{non-parametric}:  If every point in the spectrograph detector is surrounded by nearby calibration lines, the wavelength solution can, for example, be made simply as an interpolation of the calibration data.

The density of lines removes the need to enforce any functional form for the wavelength solution (such as a continuous eighth-order polynomial, for example).  We expand on the work of Milakovi\'{c}2020, which demonstrated the efficacy of constructing a wavelength solution as multiple, segmented polynomials.  Going non-parametric will improve calibration accuracy by not forcing the choice of a parametric form that may bias the calibration, especially when the chosen function is inappropriate (as, for example, polynomials will be at detector edges).

The second idea follows from the observation that contemporary \eprv\ instruments are incredibly stable.  Temperature-controlled, fiber-fed spectrographs vary only slightly over the night or season, and only along a small number of axes in what you might call ``calibration space'', or the (very high dimensional) space of all possible wavelength solutions.  That is, not only are the spectrographs stable, but they also have few accessible degrees of freedom.  This renders it inappropriate to fit each calibration exposure or calibrate each science exposure independently.  Instead, all the calibration data (or all the data) should be used to determine the space in which the instrument can and does vary.  Subsequent calibration work then need only determine where in the accessible part of calibration space the spectrograph existed for each exposure.

This structure is \emph{hierarchical}: the calibration data are used not just to determine the wavelength solution, but also to determine the possible space of wavelength solutions.  In the statistics literature this concept is often described as \emph{de-noising}.  Every calibration exposure contains information about every other exposure.  Thus every exposure can be improved (i.e. de-noised) with information from every other exposure.

The method we propose here---\name---embodies these ideas.
It is a non-parametric, hierarchical, data-driven model for the wavelength solution.  By being non-parametric, it delivers enormous freedom to the wavelength solution to match or adapt to any instrument or detector oddities.  By being hierarchical, it restricts that freedom tremendously, but it does so appropriately for the observed variations in the stabilized spectrograph.  \name\ is designed for temperature-controlled, fiber-fed spectrographs with good calibration sources, such as laser-frequency combs, etalons, or good arc lamps.  We have in mind the \eprv\ instruments and \eprv\ science cases, but we expect \name\ to have applications for other kinds of spectrographs in other contexts.  The motivating ideas behind this project are true for almost every astronomical spectrograph.

\section{Method} \label{sec:method}
\name\ is designed to turn a series of calibration lines with known wavelengths and well-fit detector positions, and de-noise and interpolate them into a full wavelength solution.  We operate on the two core ideas that the wavelength solution should be given enormous flexibility, but that it lives in a very low-dimensional space, where the degrees of freedom are set by the limited kinematics of the stabilized, spectrograph hardware.

We therefore assume dense enough calibration line coverage with well-fit line centers to provide some constraint on an interpolated wavelength solution across an order.  Additionally, we assume the instrument hardware is stabilized enough that the space of possible calibration states is low-dimensional.

We will pose the question of wavelength calibration in the following way.  Given an exposure $n$, and order $m$, there is a relationship between
the two-dimensional $(x,y)$-position on the detector and the
wavelength $\lambda$
\begin{equation}
\lambda(x,y,m,n) = f(x,y,m;\theta_{n})
\quad ,
\label{eq:wsol}
\end{equation}
where $\theta_{n}$ represents the parameters describing a given exposure.

Classically, pipelines employ polynomials to construct smooth wavelength solutions for each exposure.  For example, the pipeline described in \lz{Petersburg2020} sets the function $f(x,y,m;\theta_{n})$ from Equation \ref{eq:wsol} to be a 2D, 9\textsuperscript{th}-order polynomial where $\theta_{n}$ represents the polynomial coefficients, $c_{nij}$, unique to each exposure $n$.
\begin{equation}
\lambda(x,y,m,n) = \sum_{i=0}^9\sum_{j=0}^9 c_{nij}\, x^i\,m^j + \mathrm{noise}
\quad ,
\label{eq:poly_wsol}
\end{equation}
The coefficients $c_{nij}$ are then smoothed by a third-order polynomial over time.  This third-order polynomial is evaluated at the time of non-calibration exposures to re-construct a 2D, 9th-order polynomial wavelength solution for that exposure.  Each calibration image is fit to a 2D polynomial independently.

Given a stabilized instrument, however, the spectrograph should experience only low-degree variability, meaning the calibration of any image can and should be informed by the calibration of every other image.  The calibration data themselves can be used to develop a low-dimensional basis for expressing the space of possible calibrations for a stabilized spectrograph.

If the space of all calibration possibilities is in fact $K$-dimensional (where $K$ is a small integer, i.e 2 or 8 or thereabouts), and if the calibration variations are so small that we can linearize, then the function $f(x,y,m;\theta_{n})$ from Equation \ref{eq:wsol} could be replaced with a tiny model
\begin{equation}
\lambda(x,y,m,n) = g_0(x,y,m) + \sum_{k=1}^K a_{nk}\,g_k(x,y,m)
\quad ,
\label{eq:excl_wsol}
\end{equation}
where
$g_0(x,y,m)$ is the fiducial or mean or standard calibration of the spectrograph,
the $a_{nk}$ are scalar amplitudes,
and the $g_k(x,y,m)$ are basis functions expressing the ``directions'' in calibration space that the spectrograph can depart from the fiducial calibration.

The challenge is to learn these basis functions from the data and get the $K$ amplitudes, $a_{nk}$, for every exposure $n$.  There are many ways to discern these basis functions.  In this paper, we present a model using principal component analysis (PCA).  This is the correct thing to do in the limit where we have very high SNR, as is undeniably the case with calibration images.

\lz{Is this the place to mention options other than PCA? Or should we save that for the discussion?}

\subsection{De-Noising of Calibration Frames} \label{sec:denoising}
\lz{Concerns: 1) how to best phrase the problem using variables?, 2) how in-depth is too in-depth?}

Within a given range of stability, we can use all calibration images to  1) determine the space in which the instrument varies and 2) where in the accessible calibration space the spectrograph existed for each exposure.  Say we have $N$ calibration exposures, $exp_n$.  The first task is to identify the full list of lines we can expect to find in a calibration image.  A line, $l_{m,\lambda}$ is uniquely defined by a combination of order, $m$, and ``true" or theoretical wavelength, $\lambda$. 

For each exposure, $exp_n$, every line, $l_{m,\lambda}$, has an associated fitted detector position, $x_{n,l}$, for example x-pixel in an 2D extracted order.  Fitted line centers that are missing from an exposure (e.g. because the fit failed due to noise, the line is not in its usual order, etc.) can be assigned a $NaN$ for that order instead.  Let there be $M$ lines per exposure.  We can therefore construct a $N \times M$ matrix of line positions for each of $M$ lines for each of $N$ exposures.

The average line position in every exposure represents the fiducial, or standard calibration of the spectrograph, $g_0(x,y,m)$.  We apply principal component analysis to the difference between this fiducial calibration and each individual line position.  The returned principal components serve as basis functions,  $g_k(x,y,m)$, expressing the possible deviations of the spectrograph from this fiducial calibration.  The magnitude of each principal component for each exposure, $a_{nk}$, represents the scalar amplitude of these deviations for each exposure.  We can now use a small number, $K$, of principal components to reconstruct a de-noised version of the line positions as formulated in Equation \ref{eq:excl_wsol}.

Missing line center measurements are replaced with de-noised estimates.  We do this iteratively until the estimates of missing line centers change by less than 0.01\%.  This process can be repeated on line centers deemed as outliers by some metric, to account for lines that may have been mis-identified or mis-fit.  The principal components from the final iteration are used to define the spectrograph's calibration space while the associated magnitudes pinpoint where in that calibration space the spectrograph is for each calibration exposure.

\begin{algorithm}
\SetAlgoLined
\textbf{Inputs}: $m$, order of each line; $\lambda$, assigned wavelength of each line; $[x_{n,l}] $, matrix of pixel line position of each line $l$ and for each exposure $n$\;
\While{change in missing or outlier line centers $>$ 0.01\%}{
	$g_0(x,y,m) = \overline{x_{n,l}]}$\;
	find $U, \Sigma, V$ s.t. $U\Sigma V^* = (x_{l,n}-g_0(x,y,m))$\;
	let $a_{n,k} = U\cdot \Sigma$ and $g_k(x,y,m) = V$\;
	$\lambda(x,y,m,n) = g_0(x,y,m) + \sum_{k=1}^K a_{nk}\,g_k(x,y,m)$ for $K=6$\;
	$[x_{n,l}] = \lambda(x,y,m,n)$ where $[x_{n,l}]$ was initially $NaN$
	}
\caption{De-Noising}
\end{algorithm}

\subsection{?Interpolating Calibration Frames?}
 \label{sec:interp_time}
The magnitude of each principal component can be interpolated with respect to housekeeping data, such as time, temperature, telescope position, etc. to determine different calibration states of the spectrograph.  The choice of what to interpolate with respect to depends on the dominant contribution to variation in the instrument.

As an example, we can interpolate the magntiudes of the principal components with respect to time to the midpoint time of non-calibration exposures to determine the calibration state of the spectrograph for that exposure.  Using interpolated magnitudes, $a'_{nk}$ and the basis vectors, $g_k(x,y,m)$, returned by the de-noising process, we can construct a set of calibration lines, $\lambda(x,y,m,n)$, for any exposure as prescribed in Equation \ref{eq:excl_wsol}.


\subsection{Interpolating a Wavelength Solution} \label{sec:interp_wsol}
We now have the power to construct a set of calibration lines, $\{l_{m,\lambda}\}$, each with an assigned, known wavelength for any exposure.  To construct a wavelength solution, we need only interpolate the known wavelengths over the exposure specific line centers.  For instance, interpolating the known wavelengths vs. line centers onto every integer x will generate wavelengths for each pixel in an order.

\begin{algorithm}
\SetAlgoLined
\textbf{Inputs}: $g_0(x,y,m)$, the fiducial calibration of the spectrograph; $a_{n,k}$, magnitudes of the principal components for each exposure; $g_k(x,y,m)$, basis vectors spanning the calibration space of the spectrograph; $t_n$, time of each exposure; $t'$, time we want wavelengths for\;
Interpolate $a_{n,k}$ with respect to time to determine $a_{n,k}'$ for time $t'$\;
Calculate pixel line positions for time $t'$ by evaluating\;
$\lambda(x,y,m,n)' = g_0(x,y,m) + \sum_{k=1}^K a_{nk}'\,g_k(x,y,m)$ for $K=6$\;
\For{each unique $m$}{
	interpolate ${l_n,m}$ onto pixels, $x'$, that we want wavelengths for\;
	}
\caption{Generating a Wavelength Solution}
\end{algorithm}


\section{Data} \label{sec:data}
We tested \name\ results using data from \expres\, the EXtreme PRecison Spectrograph.  \expres\ is an environmentally-stabilized, fiber-fed, $R\sim137,000$, optical spectrograph (\lz{Jurgenson2016, Blackman2020}).  \expres\ has two different wavelength calibration sources, a ThAr lamp and a Menlo Systems laser frequency comb (LFC, e.g. \lz{Wilken2012, Molaro2013, Probst2014}).

Rather than using a simultaneous calibration fiber, two to three LFC exposures are interspersed between science exposures every ~15-30 minutes.  ThAr exposures are taken at the beginning and end of each night.  All calibration data is taken through the science fiber, thereby traveling through the same optics and hitting the same pixels as all science observations.

The results and discussion presented here are based off of 1227 LFC exposures taken between October 14 and December 18, 2020 on 29 unique nights.  LFC lines cover 40 echelle orders, which contain 19203 lines.  Though we primarily work with LFC data, we at times discuss applications to arc lamps.  These results use 78 ThAr exposures that were taken on the same nights.  ThAr lines cover all 86 extracted orders of \expres, which includes 5295 lines.

 \name\ proceeds from a given list of orders, line wavelengths, and pixel positions of each line for each exposure.  We, therefore, use line positions generated by the pre-existing \expres\ pipeline, as describe in \lz{Petersburg2020}.
 
 A ThAr wavelength solution is generated from each ThAr exposure using the IDL code \texttt{thid.pro}, developed by Jeff Valenti.  This code identifies ThAr lines by matching lines in an exposure against a line atlas. 
 
 Flat-relative, optimally extracted LFC data is background-corrected using a univariate spline.  Each peak in an order is then fit to a Gaussian.  The mean of the fitted Gaussian is taken to be the center of the line.  For each line, the ThAr wavelength solution is used to estimate the mode number of a line.  The precise wavelength is then calculated using
 \begin{equation}
 f_n = n \times  f_r + f_0
 \label{eq:lfc}
 \end{equation}
 where the repetition rate, $f_r$, is known from design of the LFC, and the offset frequency, $f_0$, has been determined experimentally.  the pre-existing \expres\ pipeline then generates wavelength solutions on a night-by-night basis as described in Section \ref{sec:method}.
 
 \lz{(Mention degeneracy with prescribed line center and wavelength solution here?  Or in discussion?)}
 
 We satisfy the assumption of stability needed for \name\ by using exposures from only one ``instrumental epoch" of \expres.  \expres\ exposures are separated into different instrumental epochs that correspond to atypical hardware changes--changes that altered the position of the echellogram on the detector, the shape of the instrumental PSF, or the calibration sources.  Significant instrumental changes break the assumption of a stabilized instrument with only low-dimensional variations.  We therefore treat each instrument epoch independently.


\section{Tests}
We performed a series of validation tests to test 1) the de-noising step (\textsection \ref{sec:denoising}, \textsection\ref{sec:interp_time}), 2) the interpolated wavelength solution (\textsection \ref{sec:interp_wsol}), and 3) the line density for which an interpolated wavelength solution is justified.  These tests center on the idea of predicting wavelengths for calibration lines, and comparing the predicted wavelength for each line to the assigned wavelength.

We compare the residuals of an \name -generated wavelength solution to wavelengths generated by a classic, parametric method.  Lastly, we compare RVs from pipelines employing \name\ vs. a classic wavelength calibration method.

\subsection{Test of De-Noising}\label{sec:test-denoising}
To test how well \name\ recreates calibration lines for non-calibration exposures, we employed \name\ on 90\% of all LFC exposures.  This means the basis vectors, $g_k(x,y,m)$,  and weights, $a_{nk}$, were constructed using only information from 90\% of all exposures.  We used the results to predict wavelengths for the lines in the remaining 10\% of calibration exposures.


\subsection{Test of Interpolation}\label{sec:test-interp}
To test how well an interpolated wavelength solution can predict wavelengths, we employed \name\ on all LFC exposures with 
of lines removed.  The resultant basis vectors, $g_k(x,y,m)$,  and weights, $a_{nk}$, therefore have no information about the excluded lines.  We then predict wavelengths for the excluded lines and compare these predictions to their assigned wavelengths.


\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{Figures/lfcgood.png}
\caption{\lz{Results of validation tests.  Histogram of residuals?  Scatter plot colored by residual value?}}
\label{fig:testHists}
\end{figure} 


\subsection{Test of Line Density}\label{sec:test-density}
The move to an interpolated wavelength solution is driven by the assumption that a high density of calibration lines should allow for more freedom in the resultant wavelength solution.  We test this assumption by employing \name\ on ThAr exposures.  We then use the results to predict wavelengths for the completely independent LFC exposures taken during the same range of time.

\subsection{Classic Model vs. \name } \label{sec:test-classic}
To compare the classic, polynomial-driven method of wavelength calibration and an interpolated wavelength solution, we focus on one exposure of LFC lines.  Within this exposure, we separate the even and odd lines.  We then construct a wavelength solution using only the odd lines and use it to predict wavelengths for the even lines.

In the case of the classic model, this meant fitting a polynomial to just the odd lines and evaluating it at the position of the even lines.  In the case of an interpolated model, this mean interpolating the wavelengths of the odd lines onto the position of the even lines.  We then constructed a wavelength model using only the even lines and used it to predict wavelengths for the even lines.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Figures/parametricTest.png}
%\includegraphics[width=\textwidth]{Figures/parametricTest.pdf}
\caption{Comparison of residuals to a parametric model (left-most) vs. a non-parametric model (right-most).  In each case, the color bar shows the difference between the predicted wavelength and the true wavelength for each line in units of $/mps$.  The center plot shows a histogram of the residuals of the polynomial fit (blue) and the non-parametric fit (green).  Medians of the residuals for each model are given in the top-right corner of the center plot in their corresponding colors.}
\label{fig:comparison}
\end{figure*}

The results to this test are shown in Figure \ref{fig:comparison}.  For each method, we plot each line with respect to its order (y-axis) and x-pixel on the detector (x-axis).  Each marker is colored by the difference between the predicted and assigned wavelength for that line in units of $\mps$.

The residuals of the classic model is shown in the left-most plot of Figure \ref{fig:comparison}; the residuals of the interpolated model is shown in the right-most plot.  The center plot is a histogram of the residuals for both methods.

The residuals of the interpolated model show much less structure, have a smaller spread, and are more symmetric about zero.  We believe the flexibility of the interpolated model was able to account for high-order detector defects, which can be seen in the structure in the residuals of the classic, smooth, polynomial model.

\subsection{Impact on Radial Velocity Measurements}\label{sec:test-rv}
\lz{Debra and RP currently working through this.  Results seem good.  Will ask them at next group meeting what we should include here}


\section{Choice Choices} \label{sec:choices}
This implementation of \name incorporated determining various global variables and methods that we believe are or are close to optimal for constructing a high-fidelity wavelength solution.  This section will describe each choice and the associated decision-making process.

\subsection{Value of K}
The value of $K$ represents the dimensionality of the calibration space within which the spectrograph lives.  In practice, it is the number of principal components, or basis vectors, used to reconstruct the de-noised line centers.  $K$ needs to be large enough so that all variability in the spectrograph is captured.  Too large, however, and the reconstruction will incorporate noise, thereby defeating the purpose.

We settled on a $K$ value of 6.  Figure \ref{fig:pcLfc} shows the first 6 principal components constructed using LFC lines.  In both cases, there is clear structure in the first and second principal components.  The diagonal-banding structure seen in components 3, 5, and 6 we believe are capturing detector fabrication defects.  Components 3 and 4 show aberrant behavior on the edges of the two bluest orders, where lower signal results in more variation in the line fits.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Figures/pcsLfc6.png}
%\includegraphics[width=\textwidth]{Figures/pcsLfc6.pdf}
\caption{The first six principal components constructed using LFC lines.  There is clear structure in the first two principal components, though no order information is given to the PCA.  Principal components 3, 5 and 6 show some slight diagonal-banding structure, which we believe to be due to detector defects.  There is aberrant behavior in the bluest lines of the bluest two orders, likely from variation introduced by more uncertain line fits.}
\label{fig:pcLfc}
\end{figure*}

In deciding a K value, we ran validation tests of denoising (as described in Section \ref{sec:test-denoising}) for K values spanning from 2 to 512.  One would expect the returned wavelengths to get increasingly more accurate with larger K until components that represent only noise are incorporated.  We found that the returned wavelengths are most accurate with a K value of 32.  However, the improvement is slight with K values greater than 6.  Comparisons of wavelengths returned by a K=6 model vs. a K=32 model show significant differences for less than 10, bluer lines.

\subsection{Interpolation of $a_{nk}$ with Respect to Time}
Figure \ref{fig:nightlyVariation} shows the amplitude of the first and second principal component with respect to time on the left.  Though there exists a complex overall shape to the amplitudes with respect to time, a clear linear trend exists within each night.  This is shown by the right plots in Figure  \ref{fig:nightlyVariation}.  As the beginning-of-night and end-of-night calibration sets always include LFC exposures, we use a simple linear interpolation to interpolate principal component amplitudes with respect to time.

\lz{Yeah, the right-figure is kind of a mess, isn't it?}

We differentiated eras of stability by where the principal component amplitudes showed large variation.  The analysis was done by eye here, though many algorithms for change-point detection exist in the literature \lz{(Am I going to have to cite things?)}.  We were very conservative with defining eras of stability to ensure that within each era, the calibration-space of the instrument is low-dimensional as assumed.

\begin{figure*}[h!]
\centering
\includegraphics[width=0.56\textwidth]{Figures/pcA_lfc.png}
\includegraphics[width=0.42\textwidth]{Figures/pcAs_byDay.png}
\caption{Amplitude of the first two principal components show as a function of time (left) or fraction of a day (right).  The top plots shows the amplitudes for the first principal component while the bottom plot shows the amplitudes for the second principal component.  Points mark the times at which we have calibration data.  Lines show the result of a linear interpolation.  In the right plot, the principal component amplitudes have been artificially offset by the median amplitude for each day so that all days are roughly on the same scale.  Points are colored by the MJD of each exposure.}
\label{fig:nightlyVariation}
\end{figure*} 

\subsection{Interpolation of Wavelengths with Respect to Pixel}
Interpolation of wavelengths over pixel  is done order by order using a Piecewise Cubic Hermite Interpolating Polynomial (PCHIP) interpolator.  This interpolation incorporates the flexibility needed to model the changing dispersion of the spectrograph across an order along with any detector defects while also enforcing monotonicity, which we know must be true across any one order.

Due to the dispersion intrinsic to echelle spectrographs, the wavelength change between pixels grows greater with greater wavelengths.  This means that the function of wavelength vs. pixel across an order will always be monotonically increasing and concave down everywhere.  Consequently, linear interpolation of this function will systematically give erroneously low values.

A more classic cubic spline interpolation can run into issues with arc lines, which are irregularly spaced or even blended and so can appear very close together.  Very close lines cause huge deviations from the correct wavelengths, as seen in the green line of Figure \ref{fig:xinterp}..

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{Figures/intpx_tests.png}
\caption{Results from different interpolation schemes over pixels.  ThAr lines, shown as pink points, are used to construct a wavelength solution that is then evaluated at each LFC line, shown as black lines.  Histograms of the residuals for each method is shown on the right.  The linear interpolation model (yellow) of this concave down relation is asymmetric with disproportionately positive residuals.  The popular cubic spline residuals (green) has very spread out residuals and shows huge deviations following a ThAr line blend at pixel \lz{(tkWill Find)}.  The residuals from an interpolation scheme that incorporates smoothing (blue) are non-Gaussian with a plateau about zero, as it chronically over-estimates the middle of the order and under-estimates the edges (not shown).  Residuals from the PCHIP interpolator (orange) has the strongest peak at 0 and appears Gaussian.}
\label{fig:xinterp}
\end{figure*} 

These huge digressions can be avoided by allowing for some smoothing in the interpolation.  In Figure \ref{fig:xinterp}, we show an example in blue using scipy's Univarate Spline method.  While the result appears to follow the calibration lines much better, the smoothing ultimately causes larger residuals that are spatially correlated (Fig. \ref{fig:xinterp}, right).

For all orders, the edges will be overestimated while the middle will be underestimated.  This indicates that the smoothing is causing the resultant wavelength solution to underestimate the curvature of the pixel-wavelength relation, resulting in similar issues as with an inflexible, parametric model.  This enforces a smoothness we have no reason to believe is true for the data.

We instead turn to the PCHIP interpolator, which damps down huge deviations in the traditional cubic spline by requiring the resulting interpolated function to be monotonic.  This is a constraint we know is true for any one order.  In Figure \ref{fig:xinterp} we show that using the PCHIP interpolator returns the lowest residuals.

\subsection{Order of Interpolation}
Why do we interpolate in time first and then in x?

\lz{We talked about including this discussion, but there is no real reason for doing things in this order except it is a bit faster.}


\section{Discussion} \label{sec:discussion}
The endeavor to reach $0.1\,\mps$ precision necessitates improvements on all levels of spectroscopy, from the instrumentation to the data analysis.  In this paper, we present \name\, a hierarchical, non-parametric model for wavelength calibration.  

Starting with a list of calibration lines with known wavelengths and well-fit line centers for each calibration exposure that faithfully represent the calibration state of the spectrograph, \name\ will de-noise and interpolate the given lines into a full wavelength solution.  We show that it returns more accurate wavelength predictions by a factor of 10 than classic, parametric methods.  Using \name\ wavelengths reduced the RMS in a planet fit to derived RVs by \lz{some percent hopefully probs}.

\name\ levies the incredible stability of contemporary EPRV instruments and high density of lines made available by new calibration sources, such as LFCs and etalons, to achieve more accurate wavelengths.  Denser calibration lines allow us to move to more flexible, interpolated wavelength solutions.  Stabilized spectrograph hardware with few degrees of freedom allow us to use all calibration images in a given generation of stability to constrain the accessible calibration space of the spectrograph.

\lz{Choices?}

An advantage of applying PCA to line positions from all LFC exposures is the ability to isolate exposures that exhibit errant variation, which is typically associated with flawed exposures.  This allowed us to quickly vet for problematic LFC exposures, which otherwise would have required visual inspection of all 1200+ LFC exposures.  In a classic framework where each calibration exposure is treated independently, these aberrant exposures would likely have persisted undetected and could potentially sway an entire night's of wavelength solutions.

On the other hand, the ability of PCA to pick up on any and all variation, regardless of source, also makes \name\ very sensitive to errors in line fitting.  For example, we have seen that lower-signal lines that are harder to fit faithfully will have greater variety in returned line positions, which is in turn captured by the PCA.  High-fidelity line positions are essential to ensure the PCA is capturing variations in the spectrograph's calibration rather than changes in how well a line can be fit.

\name\ can be applied to any data that contains information about the calibration state of the spectrograph.  For example, though LFC and ThAr exposures are used as an example in this paper, \name\ would work exactly the same for an etalon or any other arc lamp with a list of lines and assigned wavelengths.

Furthermore, once we have defined a calibration space that captures all possible degrees of freedom for a stabilized spectrograph, then all exposures taken with the spectrograph contains information about where the spectrograph lies within that calibration space.  This means that science exposures themselves could also be used to determine the calibration state of an instrument, negating the need for any additional wavelength-calibration source beyond defining the calibration space.

We have described only one, fairly simplistic implementation of \name\ here.  There are many options for both the de-noising and interpolation steps.  \lz{(Maybe something to discuss at a future meeting; I'm not actually entirely clear on other possibilities)}.  For instance, a Gaussian process model could be constructed to determine wavelengths in place of a spline. \lz{(?)}  \lz{Something about going Bayesian?}

\lz{(Not sure where this belongs, honestly.)}  We caution that with any wavelength solution, there is a perfect degeneracy between what is defined as the ``center of the line'' and the resultant wavelength solution.  If, for example, a cross correlation method is used to extract RVs from the data, a systematic difference may be introduced depending on the definition of the center of the line.  In principle, the best way to mitigate this error would be to use a calibration source that looks like a star.

\name\ is designed and optimized for extreme-precision instruments.  We believe that by implementing \name, the error from wavelength calibration is rendered negligible in the face of other error sources.




\pagebreak
\section{Graveyard of Rejected Figures}
I will mine these for caption material later before their final death.

\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{Figures/lfcgood.png}
\caption{\lz{Eventually results from the tests actually described in this paper?}.  Residuals of different wavelength calibration tests.  Shown as a black outline is the distribution of residuals from the self test described in Section \ref{sec:test-self}, which incorporates errors in the fitted line centers and interpolating with respect to pixel.  Over-plotted in blue are the residuals from the training and validation test described in Section \ref{sec:test-trainNvalid}, which builds on the self tests by additionally incorporating possible errors from interpolating line positions through time.\
\lz{Considering adding a histogram of line center errors to show it is on par?  i.e. the limitation is the line fit.  What other information is needed?  i.e. the median/mean or spread?}}
\label{fig:testHists}
\end{figure} 

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{Figures/allHists.png}
\caption{\lz{This and the previous histogram are eventually going to be one histogram, right?} Histogram of residuals for different wavelength calibration methods.  The parametric (blue) and non-parametric (green) histograms are the same as described in Figure \ref{fig:parametricTest}.  In orange is the histogram of residuals to the full Excalibur model, which includes the hierarchical component.  The median of the residuals for each model is given in the top-right corner in each model's corresponding color.}
\label{fig:allHists}
\end{figure} 

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{Figures/waveResids.png}
\caption{\lz{(Good companion figure to the line density discussion, but should also have another figure that has a bigger picture)} Comparisons of parametric and non-parametric wavelength solutions using either just ThAr lines or LFC lines. Left: the difference between each wavelength solution evaluated at the position of the LFC lines and the assigned wavelength of that LFC line.  The right plot is simply a zoom in of the left plot.  Note the large deviation when trying to implement Excalibur with just ThAr lines, which are much fewer in number.  The classic, polynomial  fit exhibits similar residuals regardless of whether the set of ThAr lines or LFC lines are used.  Excalibur using LFC lines exhibit the lowest residuals.}
\label{fig:waveResids}
\end{figure*} 

\end{document}
