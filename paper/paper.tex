% This document is part of the EPRVCalibration project
% Copyright 2019 the authors. All rights reserved.

\documentclass[12pt, letterpaper]{article}

% typesetting words
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\expres}{\project{\acronym{EXPRES}}}
\newcommand{\name}{\project{NameOfThis}}

% margins and page setup, etc
\addtolength{\textheight}{1.00in}
\addtolength{\topmargin}{-0.50in}
\sloppy\sloppypar\raggedbottom\frenchspacing

\begin{document}

\section*{\raggedright%
\name:
A non-parametric, hierarchical model for a laser-comb-calibrated spectrograph}

\noindent
\textbf{Lily~Zhao} (Yale) (Flatiron),
\textbf{David~W~Hogg} (NYU) (MPIA) (Flatiron),
\ldots and others\ldots

\paragraph{Abstract:}
There have been many hardware improvements in polygonal-fiber-fed,
temperature-controlled, laser-frequency-comb or etalon-calibrated,
high-resolution, extreme-precision radial-velocity spectrographs.
Have the calibration methodologies improved to match?
Here are three relevant observations:
The first is that the calibration lines or spots from an etalon or
comb fill the spectral range with dense calibration points.
The second is that the spectrograph lives in a stabilized,
climate-controlled environment, in which the full optical system and
detectors will only vary within a tiny range of configurations or
settings.
The third is that---given this stability--- every calibration image
ever taken is relevant to every science exposure ever taken; there is
no reason to calibrate every exposure independently.
The calibration methodology we propose here---\name---addresses these
three problems by going non-parametric (no more polynomials!) and then
reducing dimensionality with a robust principal-component method.
We demonstrate the success of this method with data from the
\expres\ spectrograph.
We find... [results and so on].

\section{Introduction}

\section{Method}

The idea is that the wavelength solution is going to live in a
low-dimensional space, where the degrees of freedom are set by the
degrees of freedom of the spectrograph hardware.

First, some definitions.
The way we think about this is the following:
Given an exposure $n$, and order $m$, there is a relationship between
the two-dimensional $(x,y)$-position on the detector and the
wavelength $\lambda$
\begin{equation}
\lambda(x,y,m,n) = f(x,y,m;\theta_{n})
\quad ,
\end{equation}
where $\theta_{n}$ is a big blob of parameters for this exposure.
This is not precisely how the \expres\ team thinks about the problem,
but it isn't far off.
For now we are going to ignore that fact, but what we say
could be adapted to any \expres\ conventions.
Right now in the \expres\ pipeline the $\theta_{n}$ comprises the
amplitudes of some 9th-ish-order polynomial in $x$ and $n$, possibly
ignoring $y$?
If so, they (may?) have
\begin{equation}
\lambda(x,y,m,n) = \sum_{i=0}^9\sum_{j=0}^9 c_{nij}\, x^i\,m^j + \mathrm{noise}
\quad ,
\end{equation}
where the $c_{nij}$ are coefficients unique to exposure $n$, and
found for exposure $n$ without regard to any other exposure $n'$.
Further, they may be finding the coefficients $c_{nij}$ that
minimize an objective $Q$ that is something like the L2-norm:
\begin{equation}
Q = ||\lambda(x,y,m,n) - \sum_{i=0}^9\sum_{j=0}^9 c_{nij}\, x^i\,m^j||_2^2
\quad .
\end{equation}

Even in this simple context, where every calibration image $n$ is
treated as its own unique flower, there are improvements to be
made.
For one, the sums should not be from 0 to 9, but instead
\begin{equation}
\sum_{i=0}^9\sum_{j=0}^{9-i}
\quad ,
\end{equation}
because that is the definition of 9th order.
If we are right about this, the fit could be taken to 13th order to obtain
much more expressiveness with (nearly) the same number of fit coefficients.
For another, the objective function could be made soft to permit
catastrophic outliers without destroying the fit.
We might recommend the iteratively reweighted least squares (IRLS).
This would make the fitting more robust.
For yet another, there are rescaling issues for the products $x^i\,m^j$ to
protect the fitting from near-singularities or bad conditioning of the
linear-algebra operators.

Of course the main thing we want to do is go hierarchical!
In this case, the calibration of every image is informed by the
calibration of every other image.
The simplest form of hierarchical model is to use the calibration data
themselves to develop a low-dimensional basis for expressing the
calibration data.

If the space of all calibration possibilities is in fact
$K$-dimensional (where I am thinking of this as being a small integer,
like $K=3$ or $K=8$ or thereabouts), and if the calibration variations are so
small that we can linearize, then the function $f(x,y,m;\theta_{n})$ could
be replaced with a tiny model
\begin{equation}
\lambda(x,y,m,n) = g_0(x,y,m) + \sum_{k=1}^K a_{nk}\,g_k(x,y,m)
\quad ,
\end{equation}
where
$g_0(x,y,m)$ is the fiducial or mean or standard calibration of the
spectrograph,
the $a_{nk}$ are scalar amplitudes,
and the $g_k(x,y,m)$ are basis functions expressing the ``directions''
in calibration space that the spectrograph can depart from the
fiducial calibration.
The challenge is to learn these basis functions from the data, and get
the $K$ amplitudes $a_{nk}$ for every exposure $n$.

...

\end{document}
